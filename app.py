import streamlit as st
import openai
import tiktoken
import matplotlib.pyplot as plt
import pandas as pd
from collections import Counter
import re
import json
from typing import List, Tuple
import os
from dotenv import load_dotenv
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import html # <--- AJOUTER CET IMPORT
import time

# Charger les variables d'environnement
load_dotenv()
# R√©cup√©ration s√©curis√©e de la cl√©
openai.api_key = st.secrets["OPENAI_API_KEY"]

# Ou pour les nouvelles versions d'OpenAI :
from openai import OpenAI
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

class TextAnalyzer:
    def __init__(self, openai_api_key: str):
        # Configuration OpenAI - Focus sur GPT-4o-mini uniquement
        self.client = openai.OpenAI(api_key=openai_api_key)
        self.model = 'gpt-4o-mini'  # Mod√®le unique pour la p√©dagogie
        self.encoding_name = 'o200k_base'
    
    def analyze_sentence(self, sentence):
        """Analyse compl√®te d'une phrase"""
        cleaned_sentence = re.sub(r'[^\w\s]', '', sentence.lower())
        words = cleaned_sentence.split()
        
        analysis = {
            'original': sentence,
            'cleaned': cleaned_sentence,
            'word_count': len(words),
            'char_count': len(sentence),
            'unique_words': len(set(words)),
            'words': words
        }
        
        return analysis
    
    def tokenize_sentence_openai(self, sentence):
        """Tokenisation avec GPT-4o-mini uniquement"""
        try:
            encoding = tiktoken.encoding_for_model(self.model)
            tokens = encoding.encode(sentence)
            
            token_strings = []
            for token in tokens:
                try:
                    token_bytes = encoding.decode_single_token_bytes(token)
                    token_string = token_bytes.decode('utf-8')
                except UnicodeDecodeError:
                    token_string = f"<bytes:{token_bytes.hex()}>"
                token_strings.append(token_string)
            
            decoded_text = encoding.decode(tokens)
            
            return {
                'model': self.model,
                'encoding': self.encoding_name,
                'tokens': tokens,
                'token_strings': token_strings,
                'token_count': len(tokens),
                'decoded_text': decoded_text,
                'is_lossless': decoded_text == sentence
            }
        except Exception as e:
            return {'error': str(e)}
    
    def get_important_words_gpt(self, sentence):
        """Analyse de tous les mots de la phrase avec GPT-4o-mini"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "Tu es un expert en analyse linguistique. Analyse la phrase et donne un score d'importance (0-1) pour CHAQUE mot de la phrase, y compris les articles, pr√©positions, etc. Tous les mots doivent √™tre inclus dans l'analyse. Retourne uniquement un JSON avec format: {{\"mots\": [{{\"mot\": \"word\", \"score\": 0.95}}, ...]}}"
                    },
                    {
                        "role": "user",
                        "content": f"Analyse TOUS les mots de cette phrase : '{sentence}'"
                    }
                ],
                temperature=0
            )
            
            result = json.loads(response.choices[0].message.content)
            return result.get('mots', [])
        except Exception as e:
            st.error(f"Erreur API OpenAI pour l'analyse : {e}")
            return []
    
    def predict_next_words(self, sentence, num_words=1, top_k=5):
        """Pr√©diction de plusieurs mots suivants avec GPT-4o-mini"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": f"Tu es un expert en pr√©diction de texte. Pr√©dis les {num_words} mot(s) suivant(s) les plus probables pour compl√©ter la phrase donn√©e. Donne {top_k} options diff√©rentes et uniques avec leur probabilit√© estim√©e. Retourne uniquement un JSON avec format: {{{{ 'predictions': [{{{{ 'sequence': 'mot(s) pr√©dit(s)', 'probabilite': 0.85 }}}}, ...] }}}}"
                    },
                    {
                        "role": "user",
                        "content": f"Pr√©dis les {num_words} mot(s) suivant(s) pour : '{sentence}'"
                    }
                ],
                temperature=0.3
            )
            
            result = json.loads(response.choices[0].message.content)
            raw_predictions = result.get('predictions', [])
            
            # Post-traitement pour garantir l'unicit√© et le nombre correct de pr√©dictions
            unique_predictions_dict = {}
            for p in raw_predictions:
                sequence = p['sequence']
                probability = p['probabilite']
                # Si la s√©quence n'est pas d√©j√† vue, ou si la nouvelle probabilit√© est meilleure
                if sequence not in unique_predictions_dict or probability > unique_predictions_dict[sequence]:
                    unique_predictions_dict[sequence] = probability
            
            # Trier par probabilit√© (d√©croissante) et prendre les top_k
            # Convertir le dictionnaire en une liste de tuples (s√©quence, probabilit√©)
            sorted_unique_predictions = sorted(unique_predictions_dict.items(), key=lambda item: item[1], reverse=True)
            
            # Retourner jusqu'√† top_k pr√©dictions uniques
            return sorted_unique_predictions[:top_k]
            
        except Exception as e:
            st.error(f"Erreur API OpenAI pour la pr√©diction : {e}")
            return []
    
    def generate_continuation(self, sentence, max_length=50, num_sequences=3):
        """G√©n√©ration de suites logiques avec GPT-4o-mini"""
        # Cette fonction n'est plus directement utilis√©e par un bouton mais peut √™tre conserv√©e pour un usage futur.
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": f"Tu es un expert en g√©n√©ration de texte. Continue la phrase donn√©e de mani√®re logique et coh√©rente. G√©n√®re {num_sequences} continuations diff√©rentes d'environ {max_length} mots chacune. Retourne uniquement un JSON avec format: {{\"continuations\": [\"suite1\", \"suite2\", ...]}}"
                    },
                    {
                        "role": "user",
                        "content": f"Continue cette phrase : '{sentence}'"
                    }
                ],
                temperature=0.8
            )
            
            result = json.loads(response.choices[0].message.content)
            return result.get('continuations', [])
        except Exception as e:
            st.error(f"Erreur API OpenAI pour la g√©n√©ration : {e}")
            return []

    def generate_continuation_from_predictions(self, sentence, predictions, target_word_count=40):
        """G√©n√®re 5 textes complets d'environ target_word_count mots en utilisant les 5 mots les plus probables."""
        try:
            if not predictions or not isinstance(predictions, list):
                st.warning("Aucune pr√©diction disponible pour g√©n√©rer les textes.")
                return []
            
            top_5_sequences = [pred[0] for pred in predictions[:5]]
            
            generated_texts = []
            for seq in top_5_sequences:
                if seq:
                    first_predicted_word = seq.split()[0]
                    base_for_generation = f"{sentence} {first_predicted_word}"
                    
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {
                                "role": "system",
                                "content": f"Tu es un assistant d'√©criture. √Ä partir de la phrase fournie : '{base_for_generation}', continue d'√©crire pour produire un texte qui, au total (phrase de d√©part incluse), fait environ {target_word_count} mots. Le texte doit √™tre coh√©rent, naturel et se terminer par un point. Retourne le texte complet (phrase de d√©part + continuation)."
                            },
                            {
                                "role": "user",
                                "content": f"Phrase de d√©part : '{base_for_generation}'. Continue √† √©crire."
                            }
                        ],
                        temperature=0.7,
                        max_tokens=int(target_word_count * 1.5) 
                    )
                    
                    # La r√©ponse du mod√®le devrait maintenant √™tre le texte complet.
                    full_text = response.choices[0].message.content.strip()
                    generated_texts.append(full_text)
            
            return generated_texts
        except Exception as e:
            st.error(f"Erreur lors de la g√©n√©ration des textes √©tendus : {e}")
            return []

def create_token_visualization(tokenization_result):
    """Cr√©e une visualisation interactive des tokens avec couleurs distinctes"""
    if 'error' in tokenization_result:
        return None
    
    tokens = tokenization_result['token_strings']
    token_ids = tokenization_result['tokens']
    
    import plotly.colors as pc
    
    if len(tokens) <= 10:
        colors = pc.qualitative.Set3[:len(tokens)]
    elif len(tokens) <= 24:
        colors = pc.qualitative.Dark24[:len(tokens)]
    else:
        base_colors = pc.qualitative.Dark24
        colors = [base_colors[i % len(base_colors)] for i in range(len(tokens))]
    
    fig = go.Figure(data=[
        go.Bar(
            x=list(range(len(tokens))),
            y=token_ids,
            text=tokens,
            textposition='auto',
            hovertemplate='<b>Token:</b> %{text}<br><b>ID:</b> %{y}<br><b>Position:</b> %{x}<extra></extra>',
            marker_color=colors,
            marker_line_color='black',
            marker_line_width=1
        )
    ])
    
    fig.update_layout(
        title="üîç Visualisation des Tokens GPT-4o-mini (Couleurs Distinctes)",
        xaxis_title="Position du Token",
        yaxis_title="ID du Token",
        height=400
    )
    
    return fig

def create_attention_heatmap(important_words):
    """Cr√©e un histogramme des scores d'attention avec gradient de couleur rouge-vert,
       en s'assurant que chaque mot n'appara√Æt qu'une fois avec son score le plus √©lev√©."""
    if not important_words:
        return None
    
    # Agr√©ger les scores pour les mots dupliqu√©s, en gardant le score le plus √©lev√©
    aggregated_scores = {}
    for item in important_words:
        mot = item['mot']
        score = item['score']
        if mot in aggregated_scores:
            aggregated_scores[mot] = max(aggregated_scores[mot], score)
        else:
            aggregated_scores[mot] = score
            
    # Trier les mots par leur score d'attention (facultatif, mais peut am√©liorer la lisibilit√©)
    # Tri√© du plus important au moins important
    sorted_aggregated_scores = dict(sorted(aggregated_scores.items(), key=lambda item: item[1], reverse=True))

    words = list(sorted_aggregated_scores.keys())
    scores = list(sorted_aggregated_scores.values())
    
    if not words: # V√©rifier si apr√®s agr√©gation, il reste des mots
        return None

    colors = []
    # Les scores sont maintenant entre 0 et 1, normalis√©s par le mod√®le GPT.
    # Le gradient ira du rouge (score proche de 1) au vert (score proche de 0).
    for score in scores:
        # Rouge intense pour score √©lev√©, Vert intense pour score faible
        red = int(255 * score)        # Plus le score est √©lev√©, plus il y a de rouge
        green = int(255 * (1 - score)) # Plus le score est bas, plus il y a de vert
        blue = 0
        colors.append(f'rgb({red},{green},{blue})')
    
    fig = go.Figure(data=[
        go.Bar(
            x=words,
            y=scores,
            marker_color=colors,
            text=[f'{score:.3f}' for score in scores],
            textposition='auto',
            hovertemplate='<b>Mot:</b> %{x}<br><b>Score d\'Attention:</b> %{y:.3f}<extra></extra>'
        )
    ])
    
    fig.update_layout(
        title="üéØ Histogramme des Scores d'Attention (Rouge = √âlev√©, Vert = Faible)",
        xaxis_title="Mots",
        yaxis_title="Score d'Attention",
        height=400,
        xaxis_tickangle=-45,
        yaxis=dict(range=[0, 1])
    )
    
    return fig

def create_prediction_histogram(predictions_list, num_words_display):
    """Cr√©e un histogramme Plotly des pr√©dictions de mots avec un gradient de couleur.
    predictions_list: une liste de tuples/listes, ex: [('seq1', 0.8), ('seq2', 0.7), ...]
    """
    if not predictions_list:
        return go.Figure().update_layout(title="Aucune pr√©diction √† afficher")
    
    # Si predictions_list est un dict avec une cl√© 'error', c'est une erreur
    if isinstance(predictions_list, dict) and 'error' in predictions_list:
        return go.Figure().update_layout(title=f"Erreur: {predictions_list['error']}")

    # Extraire les s√©quences et leurs probabilit√©s de la liste
    # La liste est suppos√©e √™tre d√©j√† tri√©e par probabilit√© par predict_next_words
    # ou nous pouvons la trier ici si n√©cessaire.
    # Pour l'instant, supposons qu'elle est dans l'ordre souhait√© ou que l'ordre n'importe pas avant le tri interne.
    
    # Assurons-nous que les √©l√©ments sont des paires (s√©quence, probabilit√©)
    try:
        # Trier par probabilit√© (deuxi√®me √©l√©ment de la paire), du plus haut au plus bas
        sorted_predictions = sorted(predictions_list, key=lambda x: x[1], reverse=True)
    except (IndexError, TypeError) as e:
        return go.Figure().update_layout(title=f"Format de pr√©dictions incorrect: {e}")

    sequences_all = [item[0] for item in sorted_predictions]
    probs_all = [item[1] for item in sorted_predictions]

    if not sequences_all or not probs_all:
        return go.Figure().update_layout(title="Aucune pr√©diction valide √† afficher")

    # Limiter au nombre de mots √† afficher
    sequences_display = sequences_all[:num_words_display]
    probs_display = probs_all[:num_words_display]

    if not probs_display:
        return go.Figure().update_layout(title="Aucune pr√©diction √† afficher apr√®s filtrage")

    # G√©n√©rer les couleurs avec un gradient de rouge (plus probable) √† vert (moins probable)
    colors = []
    min_prob_display = min(probs_display) if probs_display else 0
    max_prob_display = max(probs_display) if probs_display else 1

    for prob in probs_display:
        if max_prob_display == min_prob_display:
            norm_prob = 0.5
        else:
            norm_prob = (prob - min_prob_display) / (max_prob_display - min_prob_display)
        
        red_val = int(200 * norm_prob + 55 * (1 - norm_prob))
        green_val = int(55 * norm_prob + 200 * (1 - norm_prob))
        blue_val = 50
        
        red_val = max(0, min(255, red_val))
        green_val = max(0, min(255, green_val))
        
        colors.append(f'rgb({red_val},{green_val},{blue_val})')
    
    fig = go.Figure(data=[
        go.Bar(
            x=sequences_display,
            y=probs_display,
            marker_color=colors,
            hovertemplate='<b>S√©quence:</b> %{x}<br><b>Probabilit√©:</b> %{y:.3f}<extra></extra>'
        )
    ])
    
    # Le nombre de mots pr√©dits est implicitement 1 par pr√©diction individuelle ici
    # Si num_words_predicted √©tait dans predictions_list, il faudrait l'extraire.
    # Pour l'instant, on se base sur num_words_display pour le titre.
    title_text = f"üé≤ Pr√©dictions des {num_words_display} Mot(s) Suivant(s)" if num_words_display > 1 else "üé≤ Pr√©dictions du Mot Suivant"
    
    fig.update_layout(
        title=title_text,
        xaxis_title="S√©quences Pr√©dites",
        yaxis_title="Probabilit√©",
        height=400
    )
    
    return fig

def get_token_data_for_table(tokenization_result):
    """Pr√©pare les donn√©es des tokens pour un affichage tabulaire."""
    if not tokenization_result or 'error' in tokenization_result or not tokenization_result.get('tokens'):
        return pd.DataFrame()

    tokens_ids = tokenization_result['tokens']
    token_strings = tokenization_result['token_strings']
    
    # Cr√©e un DataFrame pour une meilleure lisibilit√©
    df = pd.DataFrame({
        'ID du Token': tokens_ids,
        'Token (texte)': token_strings,
        'Position': range(1, len(tokens_ids) + 1)
    })
    return df

def create_colored_token_html(tokenization_result):
    """Cr√©e une repr√©sentation HTML de la phrase avec des tokens color√©s."""
    if not tokenization_result or 'error' in tokenization_result or not tokenization_result.get('token_strings'):
        return ""

    token_strings = tokenization_result['token_strings']
    
    # G√©n√©rer une palette de couleurs distinctes
    if len(token_strings) <= 10:
        colors = px.colors.qualitative.Plotly[:len(token_strings)] # <--- MODIFIER ICI (pc -> px)
    elif len(token_strings) <= 20:
        colors = px.colors.qualitative.Light24[:len(token_strings)] # <--- MODIFIER ICI (pc -> px)
    else: # Pour plus de 20 tokens, on cycle sur une palette plus large
        base_colors = px.colors.qualitative.Dark24 # <--- MODIFIER ICI (pc -> px)
        colors = [base_colors[i % len(base_colors)] for i in range(len(token_strings))]

    html_parts = []
    for i, token_str in enumerate(token_strings):
        color = colors[i % len(colors)] # Cycle √† travers les couleurs si plus de tokens que de couleurs
        # √âchapper les caract√®res HTML sp√©ciaux dans le token avant de l'ins√©rer
        safe_token_str = html.escape(str(token_str)) # <--- MODIFIER CETTE LIGNE (ajout de str() pour s'assurer que c'est une cha√Æne)
        html_parts.append(f'<span style="background-color: {color}; color: black; padding: 2px 5px; margin: 2px; border-radius: 3px; display: inline-block;">{safe_token_str}</span>')
    
    return " ".join(html_parts)

def reset_session_state():
    """Fonction pour r√©initialiser les parties pertinentes de st.session_state."""
    keys_to_reset = ['tokenization', 'attention', 'predictions', 'num_words_predicted_for_display', 'generated_texts']
    for key_to_del in keys_to_reset:
        if key_to_del in st.session_state:
            del st.session_state[key_to_del]
    # R√©initialiser le champ de texte
    st.session_state.input_sentence = ""

def main():
    st.set_page_config(
        page_title="Comprendre les 3 fonctions principales d'un LLM", 
        page_icon="üéì", 
        layout="wide"
    )
    
    st.markdown("""
    <div style="background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); padding: 2rem; border-radius: 10px; margin-bottom: 2rem;">
        <h1 style="color: white; text-align: center; margin: 0;">üéì Comprendre les 3 fonctions principales d'un LLM</h1>
        <p style="color: white; text-align: center; margin: 0.5rem 0 0 0;">Exploration Interactive de la Tokenisation et de l'IA G√©n√©rative</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.sidebar.header("üîë Configuration")
    api_key = os.getenv('OPENAI_API_KEY')
    
    if api_key:
        st.sidebar.success("‚úÖ Cl√© API charg√©e")
    else:
        api_key = st.sidebar.text_input("Cl√© API OpenAI :", type="password")
    
    if not api_key:
        st.warning("‚ö†Ô∏è Veuillez configurer votre cl√© API OpenAI.")
        return
    
    if 'analyzer' not in st.session_state:
        try:
            st.session_state.analyzer = TextAnalyzer(api_key)
            st.success("‚úÖ Analyseur GPT-4o-mini initialis√© !")
        except Exception as e:
            st.error(f"Erreur lors de l'initialisation de l'analyseur : {e}")
            return
    
    analyzer = st.session_state.analyzer
    
    st.markdown("### üìù Phrase √† Analyser")
    # Assurer que la cl√© existe dans session_state pour le contr√¥le et l'initialisation
    if "input_sentence" not in st.session_state:
        st.session_state.input_sentence = "les cerises sont rouges donc je vais les"

    # La variable 'sentence' r√©cup√®re la valeur actuelle de st.session_state.input_sentence
    # gr√¢ce √† la cl√©. Toute modification par l'utilisateur met √† jour st.session_state.input_sentence.
    sentence = st.text_area(
        "Entrez votre phrase :",
        key="input_sentence", 
        height=100
    )
    
    # Boutons d'action principaux (toujours visibles en haut)
    col1_main, col2_main, col3_main, col4_main, col5_main = st.columns(5)
    
    with col1_main:
        if st.button("üîç Tokeniser", use_container_width=True, key="btn_tokenize_main"):
            if st.session_state.input_sentence:
                with st.spinner("Tokenisation en cours..."):
                    st.session_state.tokenization = analyzer.tokenize_sentence_openai(st.session_state.input_sentence)
                if 'attention' in st.session_state: del st.session_state.attention
                if 'predictions' in st.session_state: del st.session_state.predictions
                if 'generated_texts' in st.session_state: del st.session_state.generated_texts
                st.rerun() # Pour afficher les r√©sultats et le bouton contextuel
            else:
                st.warning("Veuillez entrer une phrase pour la tokenisation.")
    
    with col2_main:
        if st.button("üéØ Analyser Attention", use_container_width=True, key="btn_attention_main"):
            if st.session_state.input_sentence:
                if 'tokenization' not in st.session_state or not st.session_state.tokenization or st.session_state.tokenization.get('error'):
                    st.warning("Veuillez d'abord tokeniser une phrase avec succ√®s.")
                else:
                    with st.spinner("Analyse d'attention en cours..."):
                        st.session_state.attention = analyzer.get_important_words_gpt(st.session_state.input_sentence)
                        time.sleep(0.5) # Ajout d'un d√©lai de 3 secondes pour le test
                    if 'predictions' in st.session_state: del st.session_state.predictions
                    if 'generated_texts' in st.session_state: del st.session_state.generated_texts
                    st.rerun()
            else:
                st.warning("Veuillez entrer une phrase pour l'analyse d'attention.")
    
    with col3_main:
        if st.button("üé≤ Pr√©dire Mots", use_container_width=True, key="btn_predict_main"):
            if st.session_state.input_sentence:
                if 'attention' not in st.session_state or not st.session_state.attention:
                    st.warning("Veuillez d'abord analyser l'attention avec succ√®s.")
                else:
                    with st.spinner("Pr√©diction des mots en cours..."):
                        num_words_to_predict = 1  
                        top_k_predictions = 5     
                        st.session_state.predictions = analyzer.predict_next_words(st.session_state.input_sentence, num_words_to_predict, top_k_predictions)
                        st.session_state.num_words_predicted_for_display = top_k_predictions 
                    if 'generated_texts' in st.session_state: del st.session_state.generated_texts
                    st.rerun()
            else:
                st.warning("Veuillez entrer une phrase pour la pr√©diction.")
    
    with col4_main:
        if st.button("üìù G√©n√©rer 5 Textes", use_container_width=True, key="btn_generate_texts_main"):
            if st.session_state.input_sentence:
                if 'predictions' not in st.session_state or not st.session_state.predictions:
                    st.warning("Veuillez d'abord pr√©dire les mots avec succ√®s.")
                else:
                    with st.spinner("G√©n√©ration des textes en cours..."):
                        st.session_state.generated_texts = analyzer.generate_continuation_from_predictions(st.session_state.input_sentence, st.session_state.predictions)
                    st.rerun()
            else:
                st.warning("Veuillez entrer une phrase pour g√©n√©rer les textes.")

    with col5_main:
        st.button("üîÑ Reset", use_container_width=True, key="btn_reset", on_click=reset_session_state)

    # --- Affichage des r√©sultats ET des boutons contextuels --- 

    if 'tokenization' in st.session_state and st.session_state.tokenization and not st.session_state.tokenization.get('error'):
        st.markdown("---")
        st.markdown("### üîç R√©sultats de Tokenisation")
        col1_tok_disp, col2_tok_disp = st.columns(2)
        with col1_tok_disp:
            st.markdown("#### Repr√©sentation Textuelle Color√©e des Tokens")
            token_html = create_colored_token_html(st.session_state.tokenization)
            if token_html:
                st.markdown(token_html, unsafe_allow_html=True)
            else:
                st.info("Impossible de g√©n√©rer la repr√©sentation color√©e des tokens.")
        with col2_tok_disp:
            st.markdown("#### Tableau D√©taill√© des Tokens")
            token_df = get_token_data_for_table(st.session_state.tokenization)
            if not token_df.empty:
                st.dataframe(token_df.set_index('Position'))
            else:
                st.info("Aucune donn√©e de token √† afficher dans le tableau.")
        
        if st.button("üéØ Analyser Attention", use_container_width=True, key="btn_attention_ctx_after_tokenize"):
            if st.session_state.input_sentence:
                with st.spinner("Analyse d'attention en cours..."):
                    st.session_state.attention = analyzer.get_important_words_gpt(st.session_state.input_sentence)
                    time.sleep(0.5) # Ajout d'un d√©lai de 3 secondes pour le test (pour le bouton contextuel aussi)
                if 'predictions' in st.session_state: del st.session_state.predictions
                if 'generated_texts' in st.session_state: del st.session_state.generated_texts
                st.rerun() 
            else:
                st.warning("Veuillez entrer une phrase pour l'analyse d'attention.")

    if 'attention' in st.session_state and st.session_state.attention:
        st.markdown("---")
        st.markdown("### üéØ Analyse d'Attention")
        fig_attention = create_attention_heatmap(st.session_state.attention)
        if fig_attention:
            st.plotly_chart(fig_attention, use_container_width=True)
        else:
            st.error("Impossible de g√©n√©rer l'histogramme d'attention.")
        
        if st.button("üé≤ Pr√©dire Mots", use_container_width=True, key="btn_predict_ctx_after_attention"):
            if st.session_state.input_sentence:
                with st.spinner("Pr√©diction des mots en cours..."):
                    num_words_to_predict = 1  
                    top_k_predictions = 5     
                    st.session_state.predictions = analyzer.predict_next_words(st.session_state.input_sentence, num_words_to_predict, top_k_predictions)
                    st.session_state.num_words_predicted_for_display = top_k_predictions
                if 'generated_texts' in st.session_state: del st.session_state.generated_texts
                st.rerun() 
            else:
                st.warning("Veuillez entrer une phrase pour la pr√©diction.")

    if 'predictions' in st.session_state and st.session_state.predictions:
        st.markdown("---")
        num_words_display = st.session_state.get('num_words_predicted_for_display', 5)
        st.markdown(f"### üé≤ Top {num_words_display} Pr√©dictions du Mot Suivant") 
        col_data_pred, col_viz_pred = st.columns([1, 2])
        with col_data_pred:
            if st.session_state.predictions and isinstance(st.session_state.predictions, list):
                display_data_pred = st.session_state.predictions[:num_words_display]
                if display_data_pred:
                    df_pred = pd.DataFrame(display_data_pred, columns=['S√©quence', 'Probabilit√©'])
                    st.dataframe(df_pred, use_container_width=True)
                else:
                    st.info("Aucune donn√©e de pr√©diction √† afficher dans le tableau.")    
            else:
                st.info("Format de donn√©es de pr√©diction inattendu ou vide.")
        with col_viz_pred:
            fig_pred = create_prediction_histogram(st.session_state.predictions, num_words_display) 
            if fig_pred:
                st.plotly_chart(fig_pred, use_container_width=True)
            else:
                st.error("Impossible de g√©n√©rer le graphique des pr√©dictions.")

        if st.button("üìù G√©n√©rer 5 Textes", use_container_width=True, key="btn_generate_ctx_after_predict"):
            if st.session_state.input_sentence:
                with st.spinner("G√©n√©ration des textes en cours..."):
                    st.session_state.generated_texts = analyzer.generate_continuation_from_predictions(st.session_state.input_sentence, st.session_state.predictions)
                st.rerun() 
            else:
                st.warning("Veuillez entrer une phrase pour g√©n√©rer les textes.")

    if 'generated_texts' in st.session_state and st.session_state.generated_texts:
        st.markdown("---")
        st.markdown("### üìù 5 Textes G√©n√©r√©s √† partir des Pr√©dictions")
        for i, text in enumerate(st.session_state.generated_texts, 1):
            st.markdown(f"**Texte {i}:** {text}")
    
    # Informations p√©dagogiques dans la sidebar
    st.sidebar.markdown("---")
    st.sidebar.markdown("### üìö Guide P√©dagogique")
    st.sidebar.info("""
    **√âtapes d'Analyse :**
    
    1. üîç **Tokenisation** : D√©coupage en tokens.
    2. üéØ **Attention** : Identification des mots importants.
    3. üé≤ **Pr√©diction** : G√©n√©ration des mots suivants les plus probables.
    4. üìù **G√©n√©ration de Textes** : Cr√©ation de phrases compl√®tes avec les mots pr√©dits.
    
    **Mod√®le utilis√© :** GPT-4o-mini
    **Encoding :** o200k_base
    
    **√âchelle d'Attention (Histogramme) :**
    üî¥ Rouge fonc√© = Score d'importance √©lev√©
    üü¢ Vert clair = Score d'importance faible
    """)

if __name__ == "__main__":
    main()